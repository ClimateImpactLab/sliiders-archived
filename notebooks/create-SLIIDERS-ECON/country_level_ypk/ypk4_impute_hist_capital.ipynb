{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1319c0da-f541-486b-bb36-b17b422b3551",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code for output of historical (1950-2020) country-level capital stock projections (and actual values whenever possible)\n",
    "\n",
    "Using the I-Y (investment-to-GDP) ratio projections and GDP projections in the previous notebooks, we project the capital stock values (at the country-level).\n",
    "\n",
    "## Setting\n",
    "\n",
    "### Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317fc9e-85cb-4ba2-a70c-385af99ef86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b64af-3878-402a-91ae-84d17c6ee67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sliiders import country_level_ypk as ypk_fn\n",
    "from sliiders import settings as sset\n",
    "\n",
    "## variables header\n",
    "v_ = [\"v_\" + str(x) for x in range(1950, 2020)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a028b-9a84-40bc-8245-a008be123d41",
   "metadata": {},
   "source": [
    "## Getting the investment values\n",
    "\n",
    "The investment values can be found by multiplying the GDP values ($Y_{c, t}$) with investment-to-GDP ratios (i.e., I-Y ratios and denoted $\\left(\\frac{I}{Y}\\right)_{c, t}$). This is needed as we would like to project missing capital stock values in the years 1950-2020.\n",
    "\n",
    "We will use the GDP (`cgdpo` series) in conjunction with actual + predicted I-Y ratios as current PPP values are what are used in PWT's method of finding the \"initial capital stock value\" (at 1950 or a later year that is as early as possible).\n",
    "\n",
    "### Preparations and creating current PPP, 2017 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ab6ef-1e0a-48db-9669-94f63fd4fc97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## importing GDP (cgdpo, current PPP, 2017 USD) and I-Y ratio and create investment\n",
    "histinfo = pd.read_parquet(\n",
    "    sset.DIR_YPK_INT / \"gdp_gdppc_pop_capital_1950_2020_post_ypk4.parquet\"\n",
    ")\n",
    "histinfo.loc[pd.isnull(histinfo.iy_ratio_fit), \"iy_ratio_fit\"] = 0\n",
    "histinfo[\"curr_ppp_invest\"] = histinfo[\"cgdpo_17\"] * histinfo[\"iy_ratio_fit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baecbd82-925c-45e0-aa39-e400c910a473",
   "metadata": {},
   "source": [
    "### Fetching the PPP conversion table for year-to-year conversion\n",
    "\n",
    "For the PWT method of finding the initial capital, what we want is to add year-$t$ current PPP investment (generated above) to the year-$t$ current PPP capital stock, take care of capital depreciation, and get the year-$t+1$ capital stock. However, the said year-$t+1$ value will be in year-$t$ PPP, so we need to get year-$t$-to-year-$t+1$ PPP conversion rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00755f23-899f-43e6-9aa2-4066e7b2d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ppp table for capital stock\n",
    "ppp_to_2017_K = ypk_fn.ppp_conversion_specific_year(2017, True, True, pwtvar=\"pl_n\")\n",
    "ppp_to_2017_K.loc[pd.isnull(ppp_to_2017_K.conv), \"conv\"] = 1\n",
    "\n",
    "ppp_K_yr_to_yr = ppp_to_2017_K[[\"conv\"]].rename(columns={\"conv\": \"conv_curr_yr\"})\n",
    "ppp_K_next_yr = ppp_to_2017_K[[\"conv\"]].rename(columns={\"conv\": \"conv_next_yr\"})\n",
    "ppp_K_next_yr.reset_index(inplace=True)\n",
    "ppp_K_next_yr[\"year\"] = ppp_K_next_yr[\"year\"] - 1\n",
    "ppp_K_yr_to_yr = ppp_K_yr_to_yr.merge(\n",
    "    ppp_K_next_yr.set_index([\"ccode\", \"year\"]),\n",
    "    how=\"left\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "ppp_K_yr_to_yr[\"conv\"] = ppp_K_yr_to_yr[\"conv_curr_yr\"] / ppp_K_yr_to_yr[\"conv_next_yr\"]\n",
    "\n",
    "## we don't have 2019-to-2020 rates, so we will assume that there is no PPP rate change\n",
    "ppp_K_yr_to_yr.loc[(slice(None), 2019), \"conv\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cda573-4c56-4dc6-a3c2-2c1faf55bb4f",
   "metadata": {},
   "source": [
    "## Projecting missing values of capital stock\n",
    "\n",
    "The overall methodology for projection of missing capital stock values can be summarized as follows:\n",
    "1) For countries whose information exist only in LitPop, organize 2014 data and use (estimated) investment and depreciation rate values to project 2015-2019 data.\n",
    "2) For countries whose information exist only in GEG-15, organize 2005 data and use (estimated) investment and depreciation rate values to project 2006-2019 data.\n",
    "3) After Steps 1 and 2, 2014-2019 capital stock values will be available for all countries. Turn those capital stock values into current PPP terms (for PWT10.0 ones, just use `cn`) and calculate the ratios of current-PPP capital stock to current-PPP GDP (`cgdpo`, current PPP, 2017 USD in particular).\n",
    "4) Use `k`-nearest neighbors to make unsupervised classifications of countries based on the above-calculated capital-stock-to-GDP ratios.\n",
    "\n",
    "### Preparations (importing data, cleaning to current PPP, 2017 USD)\n",
    "\n",
    "We import the `cn` (current PPP, 2017 USD) and `rnna` (constant 2017 PPP USD) capital stock series from PWT. \n",
    "\n",
    "Also, we import LitPop data which is assumed to be in constant 2014 PPP USD (and are in ones of USD). LitPop's original source data for capital is from World Bank (link [here](https://datacatalog.worldbank.org/dataset/wealth-accounting)), which multiplies 1.24 to their values to also account for land values. We want only the capital values, so land values must be removed; LitPop already has taken care of this multiplier, so it can be used as is (can be confirmed by comparing numbers in the link above).\n",
    "\n",
    "Finally, we import GEG-15 data which is assumed to be in constant 2005 PPP USD (and are in millions of USD). This data also includes land values, so we divide the values by 1.24 to acquire only the capital stock values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ab181-d963-4d78-93aa-f1928e027da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this cell if GEG has been already cleaned up at the country level\n",
    "geg_coord = pd.read_parquet(sset.PATH_GEG15_INT).rename(columns={\"iso3\": \"ccode\"})\n",
    "geg = geg_coord.groupby(\"ccode\")[\"tot_val\"].sum()\n",
    "geg = pd.DataFrame(data={\"ccode\": geg.index, \"value\": geg.values})\n",
    "\n",
    "# country-level information\n",
    "geg.to_parquet(sset.DIR_YPK_INT / \"geg-15_ctry_lv.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4235387-e433-4dac-aaa6-7c99b17772fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PWT10.0\n",
    "pwt100 = (\n",
    "    pd.read_excel(sset.PATH_PWT_RAW)\n",
    "    .rename(columns={\"countrycode\": \"ccode\"})\n",
    "    .set_index([\"ccode\", \"year\"])\n",
    ")\n",
    "capdata = histinfo[[\"cgdpo_17\", \"rgdpna_17\", \"curr_ppp_invest\", \"delta\"]].merge(\n",
    "    pwt100[[\"cn\", \"rnna\"]],\n",
    "    how=\"left\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "\n",
    "# for litpop and geg-15, we retain current PPP but adjust from current USD to\n",
    "# constant USD.\n",
    "\n",
    "# litpop\n",
    "litpop_meta = pd.read_csv(sset.DIR_LITPOP_RAW / \"_metadata_countries_v1_2.csv\").rename(\n",
    "    columns={\"iso3\": \"ccode\", \"total_value [USD]\": \"litpop_cn\"}\n",
    ")\n",
    "litpop_meta = litpop_meta[~pd.isnull(litpop_meta.litpop_cn)]\n",
    "litpop_meta[\"year\"] = 2014\n",
    "usd_14_to_17 = pwt100.loc[(\"USA\", 2017), \"pl_n\"] / pwt100.loc[(\"USA\", 2014), \"pl_n\"]\n",
    "litpop_meta[\"litpop_cn\"] = litpop_meta[\"litpop_cn\"] / 1000000 * usd_14_to_17\n",
    "litpop_meta.set_index([\"ccode\", \"year\"], inplace=True)\n",
    "\n",
    "### geg-15\n",
    "ctry_lv_geg = pd.read_parquet(sset.DIR_YPK_INT / \"geg-15_ctry_lv.parquet\").reset_index()\n",
    "ctry_lv_geg[\"year\"] = 2005\n",
    "ctry_lv_geg = ctry_lv_geg.astype({\"value\": \"float64\"}).set_index([\"ccode\", \"year\"])\n",
    "usd_05_to_17 = pwt100.loc[(\"USA\", 2017), \"pl_n\"] / pwt100.loc[(\"USA\", 2005), \"pl_n\"]\n",
    "ctry_lv_geg[\"value\"] = ctry_lv_geg[\"value\"] / 1.24 * usd_05_to_17\n",
    "ctry_lv_geg.rename(columns={\"value\": \"geg_cn\"}, inplace=True)\n",
    "\n",
    "## merging all\n",
    "capdata = capdata.merge(ctry_lv_geg, left_index=True, right_index=True, how=\"left\")\n",
    "capdata = capdata.merge(\n",
    "    litpop_meta[[\"litpop_cn\"]], left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "## we also merge the year-to-year PPP conversion rates\n",
    "capdata = capdata.merge(\n",
    "    ppp_K_yr_to_yr[[\"conv\"]], left_index=True, right_index=True, how=\"left\"\n",
    ").drop([\"index\"], axis=1)\n",
    "capdata.loc[pd.isnull(capdata.conv), \"conv\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeae094-d0d7-49b7-8ee5-433796862f75",
   "metadata": {},
   "source": [
    "Let us fill in the capital values of uninhabited areas to be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56796f-ee84-4f67-81ac-f28bcf011162",
   "metadata": {},
   "outputs": [],
   "source": [
    "## uninhabited areas\n",
    "for i in sset.UNINHABITED_ISOS:\n",
    "    capdata.loc[i, [\"cn\"]] = 0\n",
    "    capdata.loc[i, [\"rnna\"]] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8656a-c466-4edc-887a-a9e7f14026ac",
   "metadata": {},
   "source": [
    "### 2014-2020 projection for LitPop values, 2005-2020 projection for GEG-15 values, and 2020 projection for PWT10.0 (all in current PPP)\n",
    "\n",
    "#### Log-linear interpolation for LitPop and GEG-15\n",
    "\n",
    "In the case where 2014 value exists in LitPop and 2005 value exists in GEG-15, we will not try to extrapolate the 2005-2014 values via perpetual inventory method (PIM) but rather by log-linear interpolation. Note that this will only be done for countries *not* having PWT10.0 capital stock information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e4b9de-3858-42ff-b704-09340228e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the relevant ccodes\n",
    "ccodes = capdata.index.get_level_values(\"ccode\").unique()\n",
    "pwt_cc = capdata.loc[~pd.isnull(capdata.cn), :].index.get_level_values(\"ccode\").unique()\n",
    "lp_cc = (\n",
    "    capdata.loc[~pd.isnull(capdata.litpop_cn), :]\n",
    "    .index.get_level_values(\"ccode\")\n",
    "    .unique()\n",
    ")\n",
    "geg_cc = (\n",
    "    capdata.loc[~pd.isnull(capdata.geg_cn), :].index.get_level_values(\"ccode\").unique()\n",
    ")\n",
    "lp_cc = np.setdiff1d(lp_cc, pwt_cc)\n",
    "geg_cc = np.setdiff1d(geg_cc, pwt_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7d077-6e17-4a2b-8d2f-06406cc1ead3",
   "metadata": {},
   "source": [
    "We notice, however, that there are some additional *inhabited* countries or regions that are absolutely missing all (1950-2020) capital stock information. In this case, we follow LitPop and assume that the 2014 value of capital stock for these countries is **1.247240** times the 2014 value of GDP (`cgdpo_17`, in this case). We will include these in the column `litpop_cn`. We will include these in the set of LitPop countries for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2a01c-c3d5-4a3a-9417-b9f1fa371ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_k_cc = np.setdiff1d(\n",
    "    ccodes,\n",
    "    np.union1d(np.union1d(np.union1d(lp_cc, geg_cc), pwt_cc), sset.UNINHABITED_ISOS),\n",
    ")\n",
    "print(no_k_cc)\n",
    "\n",
    "litpop_ky_ratio = 1.247240\n",
    "for i in no_k_cc:\n",
    "    capdata.loc[(i, 2014), \"litpop_cn\"] = (\n",
    "        litpop_ky_ratio * capdata.loc[(i, 2014), \"cgdpo_17\"]\n",
    "    )\n",
    "\n",
    "lp_cc = np.union1d(lp_cc, no_k_cc)\n",
    "lp_geg_cc = np.intersect1d(lp_cc, geg_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12b978-f12e-41a9-b8fb-c62ca72a4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## interpolating\n",
    "capdata[\"litpop_geg_cn\"] = np.nan\n",
    "for i in lp_geg_cc:\n",
    "    val05 = capdata.loc[(i, 2005), \"geg_cn\"]\n",
    "    val14 = capdata.loc[(i, 2014), \"litpop_cn\"]\n",
    "    val05_14 = np.exp(\n",
    "        np.interp(range(2005, 2015), [2005, 2014], np.log([val05, val14]))\n",
    "    )\n",
    "    capdata.loc[(i, list(range(2005, 2015))), \"litpop_geg_cn\"] = val05_14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14c7a0-ee26-4b6c-9e4b-51e73817e58d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### PIM projection for LitPop-GEG (2014-2020), LitPop-only (2014-2020), GEG-15-only (2005-2020), and PWT10.0 (2019-2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f9c39-98ac-46c7-bc83-f9fc095c5d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_perp_inven(\n",
    "    currK_var=\"litpop_cn\",\n",
    "    currI_var=\"curr_ppp_invest\",\n",
    "    depre_var=\"delta\",\n",
    "    ppp_conv_var=\"conv\",\n",
    "    begin_end=[2014, 2020],\n",
    "    df=capdata,\n",
    "):\n",
    "    \"\"\"Using the investment values in `currI_var`, depreciation rate values in\n",
    "    `depre_var`, and capital stock values in `currK_var`, conduct the perpertual\n",
    "    inventory method to acquire capital stock values' estimates. In every step,\n",
    "    the capital values are calculated as current PPP values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    currK_var : str\n",
    "        variable name in `df` to contain known current-PPP capital stock values\n",
    "    currI_var : str\n",
    "        variable name in `df` to contain current-PPP investment values\n",
    "    depre_var : str\n",
    "        variable name in `df` to contain depreciation rate values\n",
    "    ppp_conv_var : str\n",
    "        variable name in `df` to contain the year-to-next-year conversion rates in PPP\n",
    "    begin_end : array-like of int\n",
    "        contains the year to begin the perpetual inventory method on and to end the said\n",
    "        method on\n",
    "    df : pandas DataFrame\n",
    "        containing the necessary variables (`currK_var`, `currI_var`, `depre_var`, and\n",
    "        `ppp_conv_var`) with indices `ccode` for country-code and `year`, in that order\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas DataFrame\n",
    "        containing information with the perpetual inventory method applied to produce\n",
    "        estimates for (future) capital stock values\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    newvar = currK_var + \"_proj\"\n",
    "    df[newvar] = df[currK_var].values\n",
    "    for i in range(begin_end[0], begin_end[-1]):\n",
    "        grossK = df.loc[\n",
    "            (slice(None), i), [newvar, currI_var, depre_var, ppp_conv_var]\n",
    "        ].copy()\n",
    "        grossK[\"next_year_K\"] = (\n",
    "            (grossK[newvar] + grossK[currI_var])\n",
    "            * (1 - grossK[depre_var])\n",
    "            * (grossK[ppp_conv_var])\n",
    "        )\n",
    "        grossK.reset_index(inplace=True)\n",
    "        grossK[\"year\"] = grossK[\"year\"] + 1\n",
    "        grossK.set_index([\"ccode\", \"year\"], inplace=True)\n",
    "        df = df.merge(\n",
    "            grossK[[\"next_year_K\"]], left_index=True, right_index=True, how=\"left\"\n",
    "        )\n",
    "        df.loc[(slice(None), i + 1), newvar] = df.loc[\n",
    "            (slice(None), i + 1), \"next_year_K\"\n",
    "        ].values\n",
    "        df.drop([\"next_year_K\"], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9e416-d97c-4543-bbc8-59a6fda1db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## updating litpop\n",
    "capdata = capital_perp_inven(df=capdata)\n",
    "\n",
    "## updating geg-15\n",
    "capdata = capital_perp_inven(\"geg_cn\", begin_end=[2005, 2020], df=capdata)\n",
    "\n",
    "## updating litpop-geg-15\n",
    "capdata = capital_perp_inven(\"litpop_geg_cn\", df=capdata)\n",
    "\n",
    "## updating cn for PWT10.0\n",
    "capdata = capital_perp_inven(\"cn\", begin_end=[2019, 2020], df=capdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776f721-9f65-4311-bbda-7ef6f3889455",
   "metadata": {},
   "source": [
    "#### Creating a single current PPP, 2017 USD capital stock series (`cn_extrap`) for the data so far and tagging sources\n",
    "\n",
    "Again, we prioritize PWT10.0, then LitPop-GEG-15, then LitPop, then finally GEG-15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517dc36-1d6d-4d0d-8071-08896c37f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filling in the values\n",
    "capdata[\"cn_extrap\"] = capdata[\"cn_proj\"].values\n",
    "capdata.loc[(lp_geg_cc, slice(None)), \"cn_extrap\"] = capdata.loc[\n",
    "    (lp_geg_cc, slice(None)), \"litpop_geg_cn_proj\"\n",
    "].values\n",
    "lp_only = np.setdiff1d(lp_cc, lp_geg_cc)\n",
    "capdata.loc[(lp_only, slice(None)), \"cn_extrap\"] = capdata.loc[\n",
    "    (lp_only, slice(None)), \"litpop_cn_proj\"\n",
    "].values\n",
    "geg_only = np.setdiff1d(geg_cc, lp_geg_cc)\n",
    "capdata.loc[(geg_only, slice(None)), \"cn_extrap\"] = capdata.loc[\n",
    "    (geg_only, slice(None)), \"geg_cn_proj\"\n",
    "].values\n",
    "\n",
    "## filling in the source information\n",
    "capdata[\"cs\"] = \"-\"\n",
    "capdata.loc[~pd.isnull(capdata.cn), \"cs\"] = \"PWT\"\n",
    "capdata.loc[~pd.isnull(capdata.cn_proj) & (capdata.cs == \"-\"), \"cs\"] = \"PWT_perp_inven\"\n",
    "capdata.loc[~pd.isnull(capdata.litpop_cn) & (capdata.cs == \"-\"), \"cs\"] = \"LitPop\"\n",
    "capdata.loc[~pd.isnull(capdata.geg_cn) & (capdata.cs == \"-\"), \"cs\"] = \"GEG-15\"\n",
    "\n",
    "capdata.loc[\n",
    "    ~pd.isnull(capdata.litpop_geg_cn) & (capdata.cs == \"-\"), \"cs\"\n",
    "] = \"LitPop_GEG-15_interp\"\n",
    "capdata.loc[\n",
    "    ~pd.isnull(capdata.litpop_geg_cn_proj) & (capdata.cs == \"-\"),\n",
    "    \"cs\",\n",
    "] = \"LitPop_perp_inven\"\n",
    "capdata.loc[\n",
    "    ~pd.isnull(capdata.litpop_cn_proj) & (capdata.cs == \"-\"),\n",
    "    \"cs\",\n",
    "] = \"LitPop_perp_inven\"\n",
    "capdata.loc[\n",
    "    ~pd.isnull(capdata.geg_cn_proj) & (capdata.cs == \"-\"), \"cs\"\n",
    "] = \"GEG-15_perp_inven\"\n",
    "\n",
    "capdata.loc[(no_k_cc, [2014]), \"cs\"] = \"mult_LitPop_ratio\"\n",
    "capdata.loc[(no_k_cc, list(range(2015, 2021))), \"cs\"] = \"mult_LitPop_perp_inven\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3ba4b-9631-4b67-b3b3-2d518036b63d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Finding the initial capital stock (at the year 1950)\n",
    "\n",
    "#### Grouping the countries (via $k$-means) to find the optimal rate of change of capital intensity (capital to GDP ratio) and the range of initial capital\n",
    "\n",
    "The methodology implemented in PWT (as documented in [this PWT9.1 appendix](https://www.rug.nl/ggdc/docs/pwt91_capitalservices_ipmrevision.pdf)) to estimate the initial capital stock is as follows:\n",
    "1. Set a lower bound and an upper bound of capital intensity at the initial available year ($t_0$), and multiple the year-$t_0$ value of current PPP GDP to get lower and upper bounds of year-$t_0$ capital stock estimates. In PWT9.1, the values of lower and upper bounds of year-$t_0$ capital intensities are 0.5 and 4.0.\n",
    "2. Add on investment values and account for depreciation via the perpetual inventory method (PIM) and \"grow\" the upper and lower capital stocks.\n",
    "3. Due to depreciation, there will be a year (call this $t^*$) at which the two (upper and lower) tracks of current-PPP capital become close; PWT9.1 sets the \"closeness\" as 10% (so upper-bound capital is less than 1.1 times lower-bound capital).\n",
    "4. Calculate the upper and lower capital intensities at $t^*$, and calculate the simple mean of year-$t^*$ capital intensity (denote by $\\kappa_{t_0}$).\n",
    "5. Decrease this year-$t^*$ capital intensity by per-annum capital intensity growth rate (set as $g_\\kappa=0.02$), until it reaches the initial year. So the new initial capital intensity at $t_0$ is $\\kappa_{t_0}= \\kappa_{t^*}- g_\\kappa(t^* - t_0) $.\n",
    "6. Multiply this value with the GDP at year-$t_0$ to acquire the year-$t_0$ capital stock value.\n",
    "\n",
    "While we will follow this methodology, the problem is that the capital intensity growth rate seems to vary a lot country-by-country. Further, the initial lower and upper bounds of capital intensity being 0.5 and 4.0 each does not seem to fit the PWT10.0 update. Therefore, what we will do is the following:\n",
    "\n",
    "1. Group the countries via $k$-means using their available capital intensities.\n",
    "2. For each group, find the earliest-year (preferably 1950) upper and lower bounds of capital intensity.\n",
    "3. Also for each group, find the per-annum capital intensity growth rates.\n",
    "4. Apply the above-mentioned methodology for each group, using the updated lower / upper bounds of capital intensity at the initial year (1950) and capital intensity growth rates.\n",
    "\n",
    "For grouping, we try regular $k$-means with only the years that are available currently for all countries (2014-2020) or by filling in the missing pieces using the EM algorithm. The former, in terms of balanced classification, seems to work better, so we go with the regular $k$-means methodology with $k=3$. The EM-augmented $k$-means algorithm is from this [Stack Overflow post](https://stackoverflow.com/questions/35611465/python-scikit-learn-clustering-with-missing-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a192c31-d056-4865-af91-ed9bbd39a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_missing(X, n_clusters, max_iter=10, rand_state=60607):\n",
    "    \"\"\"Perform K-Means clustering on data with missing values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        wide-format array (with each row being different countries) to conduct the\n",
    "        EM algorithm and k-means clustering on\n",
    "    n_clusters : int\n",
    "        number of clusters to form\n",
    "    max_iter : int\n",
    "        maximum number of EM iterations to perform\n",
    "    rand_state : int\n",
    "        random state, for replicability\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels : array-like of int\n",
    "        containing integer labels, based on the EM-augmented k-means algorithm, for each\n",
    "        row in the array-like `X`\n",
    "    centroid : array-like\n",
    "        containing the centroid for each of the k-means label\n",
    "    X_hat : array-like\n",
    "        copy of `X` with the missing values filled in using the EM algorithm\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize missing values to their column means\n",
    "    missing = ~np.isfinite(X)\n",
    "    mu = np.nanmean(X, 0, keepdims=1)\n",
    "    X_hat = np.where(missing, mu, X)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        if i > 0:\n",
    "            # initialize KMeans with the previous set of centroids. this is much\n",
    "            # faster and makes it easier to check convergence (since labels\n",
    "            # won't be permuted on every iteration), but might be more prone to\n",
    "            # getting stuck in local minima.\n",
    "            clus = KMeans(n_clusters, init=prev_centroids, random_state=rand_state)\n",
    "        else:\n",
    "            # do multiple random initializations in parallel\n",
    "            clus = KMeans(n_clusters, random_state=rand_state)\n",
    "\n",
    "        # perform clustering on the filled-in data\n",
    "        labels = clus.fit_predict(X_hat)\n",
    "        centroids = clus.cluster_centers_\n",
    "\n",
    "        # fill in the missing values based on their cluster centroids\n",
    "        X_hat[missing] = centroids[labels][missing]\n",
    "\n",
    "        # when the labels have stopped changing then we have converged\n",
    "        if i > 0 and np.all(labels == prev_labels):\n",
    "            break\n",
    "\n",
    "        prev_labels = labels\n",
    "        prev_centroids = clus.cluster_centers_\n",
    "\n",
    "    return labels, centroids, X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeffa752-d181-4853-a81a-bc931619ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set aside the uninhabited areas\n",
    "uninh_capdata = capdata.loc[sset.UNINHABITED_ISOS, :].copy()\n",
    "capdata = capdata.loc[\n",
    "    ~capdata.index.get_level_values(\"ccode\").isin(sset.UNINHABITED_ISOS), :\n",
    "].sort_index()\n",
    "\n",
    "## creating K-Y ratios dataset, horizontal form (for k-means)\n",
    "capdata[\"cap_intensity\"] = capdata[\"cn_extrap\"] / capdata[\"cgdpo_17\"]\n",
    "cap_intensity = ypk_fn.organize_ver_to_hor(\n",
    "    capdata,\n",
    "    \"cap_intensity\",\n",
    "    \"year\",\n",
    "    \"ccode\",\n",
    "    range(1950, 2021),\n",
    ")\n",
    "all_kys = [\"v_\" + str(X) for X in range(1950, 2021)]\n",
    "cap_intensity[all_kys] = cap_intensity[all_kys].astype(\"float64\")\n",
    "\n",
    "## we can use only the filled information; initializing clustering algorithms\n",
    "cluster_3 = KMeans(n_clusters=3, random_state=60607)\n",
    "comp_ky_s_filled = [\"v_\" + str(X) for X in range(2014, 2021)]\n",
    "cap_intensity[\"cl3\"] = cluster_3.fit(cap_intensity[comp_ky_s_filled].values).labels_\n",
    "\n",
    "## based on balanced classification, 3 seems to be the most optimal\n",
    "## with EM algorithm as well\n",
    "em_kmeans = kmeans_missing(cap_intensity[all_kys], 3)\n",
    "cap_intensity[\"cl3_em\"] = em_kmeans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848acfb-dde1-4016-9f38-97c2d9a06700",
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying to see the balancedness between regular k-means and EM-augmented version\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(cap_intensity[\"cl3\"].astype(\"int64\"))\n",
    "ax1.set_xticks([0, 1, 2])\n",
    "\n",
    "ax2.hist(cap_intensity[\"cl3_em\"].astype(\"int64\"))\n",
    "ax2.set_xticks([0, 1, 2])\n",
    "\n",
    "ax1.set_ylim([0, 160]), ax2.set_ylim([0, 160])\n",
    "ax1.set_yticks([0, 40, 80, 120, 160]), ax2.set_yticks([0, 40, 80, 120, 160])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c97da5-5b83-41fe-b6c5-805e48b16717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# em grouping gives only 1 country assigned to the final group\n",
    "cap_intensity.reset_index().groupby([\"cl3\"]).count()[[\"ccode\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb5792-53ab-4b42-8f40-5dfc38fd6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replication of Table 1 in Inklaar et al. (Intl Productivity Monitor 2019)\n",
    "rows = []\n",
    "for i in [1950, 1960, 1970, 1980, 1990, 2000, 2011, 2017]:\n",
    "    v = f\"v_{i}\"\n",
    "    row = [i, cap_intensity.loc[~pd.isnull(cap_intensity[v]), :].shape[0]]\n",
    "    row += [\n",
    "        round(cap_intensity[v].mean(), 1),\n",
    "        round(cap_intensity[v].std(), 1),\n",
    "        round(cap_intensity[v].min(), 1),\n",
    "        round(cap_intensity[v].max(), 1),\n",
    "    ]\n",
    "    rows.append(row)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.array(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498b437-8ca5-4667-9832-14d301dcd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## attaching the cluster types\n",
    "capdata = capdata.merge(\n",
    "    cap_intensity[\"cl3\"], left_index=True, right_index=True, how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85f524-75c0-47c8-9094-2e4be0a51494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_max_growthrate_by_group(\n",
    "    df=capdata, group=\"cl3\", ratio=\"cap_intensity\"\n",
    "):\n",
    "    \"\"\"By specified `group` designation, calculate the lower and upper bounds of the\n",
    "    variable `ratio` contained in DataFrame `df`, as well as the said variable's average\n",
    "    annual growth rate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        containing information about the `group` and `ratio`. Should also contain the\n",
    "        variable `year` as growth rate values are calculated yearly.\n",
    "    group : str\n",
    "        column name in `df` that represents the grouping (by k-means clustering or\n",
    "        other methods)\n",
    "    ratio : str\n",
    "        column name in `df` that represents the variable for calculating the lower,\n",
    "        upper bounds and annual growth rates\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    growth_rate_df : pandas DataFrame\n",
    "        containing, by group, the information about lower bound of `ratio` (`ky_lower`,\n",
    "        and set to be the 10th quantile from the bottom), upper bound of `ratio`\n",
    "        (`ky_upper`, and set to be the 90th quantile from the bottom), and growth rate\n",
    "        per annum of `ratio` (`ky_growth`). Also stores the grouping information in the\n",
    "        variable `cl`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    growth_rate_df = []\n",
    "    for cl in np.sort(df[group].unique()):\n",
    "        cl_df = df.loc[df[group] == cl, [ratio]].copy()\n",
    "        nona_ratios = cl_df[ratio].values\n",
    "        nona_ratios = nona_ratios[~pd.isnull(nona_ratios)]\n",
    "        cl_lower, cl_upper = np.quantile(nona_ratios, [0.1, 0.9])\n",
    "\n",
    "        cl_df = cl_df.loc[~pd.isnull(cl_df[ratio]), :].reset_index()\n",
    "        cl_growth = sm.OLS(\n",
    "            cl_df[ratio].astype(\"float64\"),\n",
    "            sm.add_constant(cl_df[[\"year\"]]).astype(\"float64\"),\n",
    "        )\n",
    "        cl_growth = cl_growth.fit().params[\"year\"]\n",
    "        growth_rate_df.append([cl, cl_lower, cl_upper, cl_growth])\n",
    "\n",
    "    growth_rate_df = pd.DataFrame(\n",
    "        np.vstack(growth_rate_df),\n",
    "        columns=[\"cl\", \"ky_lower\", \"ky_upper\", \"ky_growth\"],\n",
    "    )\n",
    "\n",
    "    return growth_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ca697-9eac-47c4-b020-aa2beea3e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## growth rates, upper and lower bounds for capital intensity\n",
    "cl_gr = calculate_min_max_growthrate_by_group(df=capdata).rename(columns={\"cl\": \"cl3\"})\n",
    "cl_gr[\"cl3\"] = cl_gr[\"cl3\"].astype(\"int64\")\n",
    "capdata = (\n",
    "    capdata.reset_index()\n",
    "    .merge(cl_gr, on=[\"cl3\"], how=\"left\")\n",
    "    .set_index([\"ccode\", \"year\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8afb79-0b5c-412b-9e7f-c88c51682ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99832dcb-0c13-446b-9a15-3fc924b3be1d",
   "metadata": {},
   "source": [
    "#### Applying PWT 9.1's method, cluster by cluster, and interpolating with the known values of capital\n",
    "\n",
    "We will also need our investment values to apply the PWT 9.1's method, so we will do so below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44211983-4acc-4563-a2db-8295f08152b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_init_k(\n",
    "    df=capdata,\n",
    "    begin_end=[1950, 2020],\n",
    "    lb=\"ky_lower\",\n",
    "    ub=\"ky_upper\",\n",
    "    gr=\"ky_growth\",\n",
    "    currK_var=\"cn\",\n",
    "    currY_var=\"cgdpo_17\",\n",
    "    currI_var=\"curr_ppp_invest\",\n",
    "    depre_var=\"delta\",\n",
    "    ytoy_ppp=\"conv\",\n",
    "    cluster=\"cl3\",\n",
    "    ub_lb_thresh=0.1,\n",
    "):\n",
    "    \"\"\"Finding the initial value of capital (at the year specified by `begin_end`)\n",
    "    based on the methdology of PWT 9.1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame to contain all necessary information (current PPP GDP, investment,\n",
    "        depreciation rates, growth rate, lower bound and upper bound for the capital\n",
    "        intensity)\n",
    "    begin_end : array-like of ints\n",
    "        array-like containing two elements - initial year and the final year to be\n",
    "        considered by the process\n",
    "    lb : str\n",
    "        column name in `df` to indicate the lower bound of capital intensity\n",
    "    ub : str\n",
    "        column name in `df` to indicate the upper bound of capital intensity\n",
    "    gr : str\n",
    "        column name in `df` to indicate the average yearly growth of capital intensity\n",
    "    currK_var : str\n",
    "        column name in `df` for current-PPP capital\n",
    "    currY_var : str\n",
    "        column name in `df` for current-PPP GDP\n",
    "    currI_var : str\n",
    "        column name in `df` for current-PPP investment\n",
    "    depre_var : str\n",
    "        column name in `df` for depreciate rate\n",
    "    ytoy_ppp : str\n",
    "        column name in `df` for year-to-next-year PPP conversion rate\n",
    "    cluster : str\n",
    "        column name in `df` for cluster (based on capital intensity values)\n",
    "    ub_lb_thresh : float\n",
    "        difference between upper- and lower-bound capital stock values to halt and\n",
    "        acquire year `tstar`\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    estimated : pandas.DataFrame\n",
    "        DataFrame with `ccode` (country code) as the index containing initial-year\n",
    "        capital stock estimations (based on the PWT 9.1 method); only contains\n",
    "        information if a country was actually missing the initial-year capital stock\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cl_df = df[[cluster, lb, ub, gr, currY_var, currI_var, ytoy_ppp, depre_var]].copy()\n",
    "    cl_df[\"low_k\"], cl_df[\"high_k\"] = np.nan, np.nan\n",
    "    for yr in range(begin_end[0], begin_end[-1]):\n",
    "        ## setting the initial year's lower and upper bound capital\n",
    "        if yr == begin_end[0]:\n",
    "            cl_df.loc[(slice(None), yr), \"low_k\"] = (\n",
    "                cl_df.loc[(slice(None), yr), [currY_var, lb]].product(axis=1).values\n",
    "            )\n",
    "            cl_df.loc[(slice(None), yr), \"high_k\"] = (\n",
    "                cl_df.loc[(slice(None), yr), [currY_var, ub]].product(axis=1).values\n",
    "            )\n",
    "        nxt = yr + 1\n",
    "        for i in [\"low_k\", \"high_k\"]:\n",
    "            cl_df.loc[(slice(None), nxt), i] = (\n",
    "                cl_df.loc[(slice(None), yr), [i, currI_var]].sum(axis=1).values\n",
    "                * (1 - cl_df.loc[(slice(None), yr), depre_var].values)\n",
    "                * cl_df.loc[(slice(None), yr), ytoy_ppp].values\n",
    "            )\n",
    "    cl_df[\"hi_lo_ratio\"] = cl_df[\"high_k\"] / cl_df[\"low_k\"] - 1\n",
    "\n",
    "    ## finding t-star, the year that high- and low-trajectories are lesser than\n",
    "    ## the threshold set by `ub_lb_thresh`\n",
    "    tstar_df = (\n",
    "        cl_df.loc[cl_df.hi_lo_ratio < ub_lb_thresh, :]\n",
    "        .reset_index()\n",
    "        .groupby([\"ccode\"])\n",
    "        .min()[[\"year\"]]\n",
    "        .rename(columns={\"year\": \"tstar\"})\n",
    "    )\n",
    "    cl_df = cl_df.merge(tstar_df, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "    ## if tstar is not acquired, get the latest year to be the tstar\n",
    "    cl_df.loc[pd.isnull(cl_df[\"tstar\"]), \"tstar\"] = begin_end[-1]\n",
    "\n",
    "    ## country-by-country calculation of initial capital for those missing them\n",
    "    init = df.loc[(slice(None), begin_end[0]), [currK_var]].copy()\n",
    "    msng_ccodes = (\n",
    "        init.loc[pd.isnull(init[currK_var]), :].index.get_level_values(\"ccode\").unique()\n",
    "    )\n",
    "    estimated = []\n",
    "    for cc in msng_ccodes:\n",
    "        ## how many years from tstar to initial year\n",
    "        tstar = cl_df.loc[(cc, begin_end[0]), \"tstar\"]\n",
    "        tstar_t0 = tstar - begin_end[0]\n",
    "\n",
    "        ## initial-year capital-to-GDP ratio\n",
    "        init_ky = cl_df.loc[(cc, [tstar]), [\"high_k\", \"low_k\"]].mean(axis=1).values[\n",
    "            0\n",
    "        ] / cl_df.loc[(cc, tstar), currY_var] - (\n",
    "            tstar_t0 * cl_df.loc[(cc, begin_end[0]), gr]\n",
    "        )\n",
    "        if init_ky < cl_df.loc[(cc, tstar), lb]:\n",
    "            init_ky = cl_df.loc[(cc, tstar), lb]\n",
    "        elif init_ky > cl_df.loc[(cc, tstar), ub]:\n",
    "            init_ky = cl_df.loc[(cc, tstar), ub]\n",
    "\n",
    "        ## initial-year capital value\n",
    "        init_K = init_ky * cl_df.loc[(cc, begin_end[0]), currY_var]\n",
    "        estimated.append([cc, init_K])\n",
    "    estimated = pd.DataFrame(\n",
    "        np.vstack(estimated), columns=[\"ccode\", \"cn_init_estim\"]\n",
    "    ).set_index([\"ccode\"])\n",
    "\n",
    "    return estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912a76a-c9d0-437b-8e27-79a68494a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## estimating the \"cn_init_estim\" (missing initial-year capital)\n",
    "cn_init_estim = find_init_k(capdata)\n",
    "\n",
    "## merging with the rest\n",
    "capdata = capdata.merge(cn_init_estim, left_index=True, right_index=True, how=\"left\")\n",
    "capdata.loc[\n",
    "    (capdata.cs == \"-\")\n",
    "    & (~pd.isnull(capdata.cn_init_estim))\n",
    "    & (capdata.index.get_level_values(\"year\") == 1950),\n",
    "    \"cs\",\n",
    "] = \"init_K_estim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc9a75d-a2aa-4ab9-9dd0-3af69deb1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## interpolating the rest, and filling the said values to cn_extrap\n",
    "msng_ccodes = (\n",
    "    capdata.loc[(~pd.isnull(capdata.cn_init_estim)), :]\n",
    "    .index.get_level_values(\"ccode\")\n",
    "    .unique()\n",
    ")\n",
    "capdata[\"cn_init_estim\"] = capdata[\"cn_init_estim\"].astype(\"float64\")\n",
    "capdata[\"cn_extrap\"] = capdata[\"cn_extrap\"].astype(\"float64\")\n",
    "for i in msng_ccodes:\n",
    "    ## initial capital that was estimated\n",
    "    init_K = capdata.loc[(i, 1950), \"cn_init_estim\"]\n",
    "\n",
    "    filled_K = capdata.loc[\n",
    "        (capdata.index.get_level_values(\"ccode\") == i)\n",
    "        & (~pd.isnull(capdata.cn_extrap)),\n",
    "        [\"cn_extrap\"],\n",
    "    ]\n",
    "    filled_yr_min = filled_K.index.get_level_values(\"year\").min()\n",
    "    filled_yr_min_K = capdata.loc[(i, filled_yr_min), \"cn_extrap\"]\n",
    "\n",
    "    interp_K = np.interp(\n",
    "        range(1950, filled_yr_min + 1),\n",
    "        [1950, filled_yr_min],\n",
    "        np.log([init_K, filled_yr_min_K]),\n",
    "    )\n",
    "    interp_K = np.exp(interp_K)\n",
    "    i_yrs = list(range(1950, filled_yr_min + 1))\n",
    "    capdata.loc[(i, i_yrs), \"cn_extrap\"] = interp_K\n",
    "    capdata.loc[(i, i_yrs[1:-1]), \"cs\"] = \"init_K_estim_interp\"\n",
    "\n",
    "## filling in information for ratio-extrapolated\n",
    "capdata.loc[(no_k_cc, 2014), \"cs\"] = \"LitPop_ratio_extrap\"\n",
    "\n",
    "capdata = pd.concat([capdata, uninh_capdata], axis=0).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3711e-a0ce-49b3-8a0c-c896fd20251e",
   "metadata": {},
   "source": [
    "We will merge the acquired result for current-PPP capital stock (and their sources) with the other historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8b379-9a7d-41c6-9f3e-b212e5c036ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "histinfo = histinfo.merge(\n",
    "    capdata[[\"cn_extrap\", \"cs\"]].rename(columns={\"cs\": \"capital_source\"}),\n",
    "    how=\"left\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d8f66-e11c-408c-9e79-deb8d9ba0972",
   "metadata": {},
   "source": [
    "## Filling in the missing `rnna` values, generating current PPP, 2019 USD capital values (`cn_19`) and constant 2019 PPP USD capital values (`rnna_19`)\n",
    "\n",
    "### For the missing `rnna` values (current PPP, 2017 USD)\n",
    "\n",
    "For these ones, we need to make sure that $rnna_{c, 2017} = cn_{c, 2017}$ for any country $c$. For the countries whose `rnna` information is missing entirely, we will use the (extrapolated) conversion rates to turn the `cgdpo` to `rnna` values. But for the countries whose `rnna` information does exist partially, we first apply the conversion rates, get `rnna` equivalents, get the growth rates of `rnna`-equivalents for the missing years, and apply them to the pre-existing `rnna` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc538be-0b1b-4c8a-ad81-a97bf5bff40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## conversion rates (PPP) attached (from current to 2017 PPP)\n",
    "histinfo = histinfo.merge(\n",
    "    ppp_to_2017_K[[\"conv\"]].rename(columns={\"conv\": \"curr_to_cnst\"}),\n",
    "    how=\"left\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "histinfo.loc[(slice(None), 2020), \"curr_to_cnst\"] = histinfo.loc[\n",
    "    (slice(None), 2019), \"curr_to_cnst\"\n",
    "].values\n",
    "histinfo.loc[pd.isnull(histinfo.curr_to_cnst), \"curr_to_cnst\"] = 1\n",
    "\n",
    "## creating `rnna equivalents`\n",
    "histinfo[\"rnna_equiv\"] = histinfo[\"cn_extrap\"] * histinfo[\"curr_to_cnst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e3be2-7035-4d5c-b24a-7120c80b02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merging the actual rnna values from PWT10.0, and detecting which are missing\n",
    "## rnna values completely\n",
    "histinfo = histinfo.merge(\n",
    "    pwt100[[\"rnna\"]], left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "\n",
    "## detecting those that have some rnna information vs. don't\n",
    "count_rnna = histinfo.reset_index().groupby(\"ccode\").count()[[\"rnna\"]]\n",
    "no_rnna = count_rnna.loc[count_rnna.rnna == 0, :].index.values\n",
    "some_rnna = count_rnna.loc[count_rnna.rnna > 0, :].index.values\n",
    "\n",
    "## filling in the information for those that absolutely do not have rnna information\n",
    "histinfo[\"rnna_extrap\"] = np.nan\n",
    "histinfo.loc[(no_rnna, slice(None)), \"rnna_extrap\"] = histinfo.loc[\n",
    "    (no_rnna, slice(None)), \"rnna_equiv\"\n",
    "].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759dfd1b-35c7-401e-9141-a1ceb9eb7b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for the partially-filled countries, fill in by using growth rates\n",
    "for cc in tqdm(some_rnna):\n",
    "    nona_yrs = histinfo.loc[\n",
    "        (histinfo.index.get_level_values(\"ccode\") == cc) & (~pd.isnull(histinfo.rnna)),\n",
    "        :,\n",
    "    ]\n",
    "    nona_yrs = nona_yrs.index.get_level_values(\"year\")\n",
    "    nona_maxyr, nona_minyr = nona_yrs.max(), nona_yrs.min()\n",
    "\n",
    "    ## copying information into the rnna_extrap column\n",
    "    histinfo.loc[(cc, nona_yrs), \"rnna_extrap\"] = histinfo.loc[\n",
    "        (cc, nona_yrs), \"rnna\"\n",
    "    ].values\n",
    "\n",
    "    ## using growth rates for extrapolation\n",
    "    rnna_1950, rnna_2020 = histinfo.loc[(cc, [1950, 2020]), \"rnna\"].values\n",
    "    if pd.isnull(rnna_1950):\n",
    "        fill_yrs = list(range(1950, nona_minyr + 1))\n",
    "        equiv = histinfo.loc[(cc, fill_yrs), \"rnna_equiv\"].values\n",
    "        actual_extrap = (equiv / equiv[-1]) * histinfo.loc[(cc, nona_minyr), \"rnna\"]\n",
    "        histinfo.loc[(cc, fill_yrs), \"rnna_extrap\"] = actual_extrap\n",
    "\n",
    "    if pd.isnull(rnna_2020):\n",
    "        fill_yrs = list(range(nona_maxyr, 2021))\n",
    "        equiv = histinfo.loc[(cc, fill_yrs), \"rnna_equiv\"].values\n",
    "        actual_extrap = (equiv / equiv[0]) * histinfo.loc[(cc, nona_maxyr), \"rnna\"]\n",
    "        histinfo.loc[(cc, fill_yrs), \"rnna_extrap\"] = actual_extrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d4c89-9483-4efe-8484-4c9f7c7a597a",
   "metadata": {},
   "source": [
    "### Creating `cn_19` and `rnna_19`\n",
    "\n",
    "Again, for these, it must be that $cn\\_19_{c, 2019} = rnna\\_19_{c, 2019}$ for all countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643385e-48c8-4f7d-91fc-e425429999ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cn_19 is created simply by chaning from USD of 2017 to USD of 2019\n",
    "usd_17_19 = pwt100.loc[(\"USA\", 2019), \"pl_n\"] / pwt100.loc[(\"USA\", 2017), \"pl_n\"]\n",
    "histinfo[\"cn_19\"] = histinfo[\"cn_extrap\"] * usd_17_19\n",
    "\n",
    "## creating rnna_19; first creating scale factors with 2019 values being 1\n",
    "rnna_17_2019_vals = (\n",
    "    histinfo.loc[(slice(None), 2019), [\"rnna_extrap\"]]\n",
    "    .reset_index()\n",
    "    .drop([\"year\"], axis=1)\n",
    "    .set_index([\"ccode\"])\n",
    "    .rename(columns={\"rnna_extrap\": \"rnna_2019_vals\"})\n",
    ")\n",
    "histinfo = histinfo.merge(\n",
    "    rnna_17_2019_vals, left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "histinfo[\"rnna_2019_scale\"] = histinfo[\"rnna_extrap\"] / histinfo[\"rnna_2019_vals\"]\n",
    "\n",
    "## multiplying the cn_19 values of 2019\n",
    "cn_19_2019_vals = (\n",
    "    histinfo.loc[(slice(None), 2019), [\"cn_19\"]]\n",
    "    .reset_index()\n",
    "    .drop([\"year\"], axis=1)\n",
    "    .set_index([\"ccode\"])\n",
    "    .rename(columns={\"cn_19\": \"cn_19_2019\"})\n",
    ")\n",
    "histinfo = histinfo.merge(\n",
    "    cn_19_2019_vals, left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "histinfo[\"rnna_19\"] = histinfo[\"rnna_2019_scale\"] * histinfo[\"cn_19_2019\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552db7d5-3e75-46f2-88e5-587bfb2ded18",
   "metadata": {},
   "source": [
    "## Creating capital and population scales, organizing the variable names, and exporting\n",
    "\n",
    "### Creating capital scale (with respect to `cn_19` of 2019) and population scale (with respect to `pop` of 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938bf65-53cf-45cb-9cc6-c82b3f9ff3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pop scale\n",
    "pop2019 = (\n",
    "    histinfo.loc[(slice(None), 2019), [\"pop\"]]\n",
    "    .reset_index()\n",
    "    .drop([\"year\"], axis=1)\n",
    "    .set_index([\"ccode\"])\n",
    "    .rename(columns={\"pop\": \"pop_2019\"})\n",
    ")\n",
    "histinfo = histinfo.merge(pop2019, left_index=True, right_index=True, how=\"left\")\n",
    "histinfo[\"pop_scale\"] = histinfo[\"pop\"] / histinfo[\"pop_2019\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca274a42-929b-4e16-8919-6de744263559",
   "metadata": {},
   "outputs": [],
   "source": [
    "## capital scale\n",
    "cn2019 = (\n",
    "    histinfo.loc[(slice(None), 2019), [\"cn_19\"]]\n",
    "    .reset_index()\n",
    "    .drop([\"year\"], axis=1)\n",
    "    .set_index([\"ccode\"])\n",
    "    .rename(columns={\"cn_19\": \"cn_2019\"})\n",
    ")\n",
    "histinfo = histinfo.merge(cn2014, left_index=True, right_index=True, how=\"left\")\n",
    "histinfo[\"rnna_19_scale\"] = histinfo[\"rnna_19\"] / histinfo[\"cn_2019\"]\n",
    "histinfo[\"cn_19_scale\"] = histinfo[\"cn_19\"] / histinfo[\"cn_2019\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945ef5d-a024-4f29-856b-704a7f1d12f1",
   "metadata": {},
   "source": [
    "### Variable name cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1074ed0c-efbe-45ae-915b-62e8942caf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "histinfo_columns = [\n",
    "    \"pop_unit\",\n",
    "    \"gdppc_unit\",\n",
    "    \"gdp_capital_unit\",\n",
    "    \"pop_source\",\n",
    "    \"gdp_source\",\n",
    "    \"iy_ratio_source\",\n",
    "    \"k_ratio_source\",\n",
    "    \"delta_source\",\n",
    "    \"capital_source\",\n",
    "    \"pop\",\n",
    "    \"pop_scale\",\n",
    "    \"rgdpna_pc_17\",\n",
    "    \"rgdpna_17\",\n",
    "    \"rgdpna_pc_19\",\n",
    "    \"rgdpna_19\",\n",
    "    \"cgdpo_pc_17\",\n",
    "    \"cgdpo_17\",\n",
    "    \"cgdpo_pc_19\",\n",
    "    \"cgdpo_19\",\n",
    "    \"iy_ratio\",\n",
    "    \"iy_ratio_fit\",\n",
    "    \"k_movable_ratio\",\n",
    "    \"k_struc_ratio\",\n",
    "    \"k_mach_ratio\",\n",
    "    \"k_traeq_ratio\",\n",
    "    \"k_other_ratio\",\n",
    "    \"delta\",\n",
    "    \"rnna_17\",\n",
    "    \"rnna_19\",\n",
    "    \"rnna_19_scale\",\n",
    "    \"cn_17\",\n",
    "    \"cn_19\",\n",
    "    \"cn_19_scale\",\n",
    "]\n",
    "histinfo_final = histinfo.copy()\n",
    "histinfo_final.rename(\n",
    "    columns={\n",
    "        \"rnna_extrap\": \"rnna_17\",\n",
    "        \"cn_extrap\": \"cn_17\",\n",
    "        \"gdp_unit\": \"gdp_capital_unit\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "## filling in the nan's with 0s\n",
    "fill0 = [\n",
    "    \"rnna_17\",\n",
    "    \"rnna_19\",\n",
    "    \"rnna_19_scale\",\n",
    "    \"cn_17\",\n",
    "    \"cn_19\",\n",
    "    \"cn_19_scale\",\n",
    "    \"pop_scale\",\n",
    "]\n",
    "for i in fill0:\n",
    "    histinfo_final.loc[pd.isnull(histinfo_final[i]), i] = 0\n",
    "\n",
    "histinfo_final = histinfo_final[histinfo_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9174477-b7d0-4553-b33c-a4cabb0321ff",
   "metadata": {},
   "source": [
    "### Exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae83a2c3-fc71-497b-b676-3bdca885a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(sset.DIR_YPK_FINAL, exist_ok=True)\n",
    "histinfo_final.to_parquet(\n",
    "    sset.DIR_YPK_FINAL / \"gdp_gdppc_pop_capital_1950_2020.parquet\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
