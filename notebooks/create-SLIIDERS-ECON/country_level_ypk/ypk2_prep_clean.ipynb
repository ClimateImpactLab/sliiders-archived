{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08e3612",
   "metadata": {},
   "source": [
    "## Preparing and cleaning files necessary for (country-level) capital stock projection workflow\n",
    "\n",
    "## Importing necessary modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9622a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7606bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from operator import itemgetter\n",
    "\n",
    "import dask.dataframe as ddf\n",
    "import dask.delayed as delayed\n",
    "import fiona\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycountry as pyctry\n",
    "from dask_gateway import Gateway\n",
    "from py7zr import unpack_7zarchive\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sliiders import country_level_ypk as ypk_fn\n",
    "from sliiders import settings as sset\n",
    "from sliiders import spatial\n",
    "\n",
    "# dask gateway setup\n",
    "gateway = Gateway()\n",
    "image_name = sset.DASK_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d76358a",
   "metadata": {},
   "source": [
    "## Maddison Project: scale change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "## original file format was excel spreadsheet, so we will read this as is\n",
    "madd = pd.read_excel(sset.DIR_YPK_RAW / \"maddison_project.xlsx\")\n",
    "\n",
    "## population is in 1000s of people; we will save it to be in millions of people\n",
    "madd[\"pop\"] = madd[\"pop\"] / 1000  ## divide by a thousand to get things in millions\n",
    "\n",
    "## GDPpc is currently in ones of USD; we want gdp to be in millions of USD\n",
    "## one USD per million people = 1 million USD per person\n",
    "madd[\"gdp\"] = madd[\"gdppc\"] * madd[\"pop\"]\n",
    "\n",
    "## indexing and exporting\n",
    "madd.rename(columns={\"countrycode\": \"ccode\"}, inplace=True)\n",
    "madd[\"gdppc_unit\"] = \"ones of USD (constant 2011 PPP USD)\"\n",
    "madd[\"gdp_unit\"] = \"millions of USD (constant 2011 PPP USD)\"\n",
    "madd[\"pop_unit\"] = \"millions of people\"\n",
    "madd.set_index([\"ccode\", \"year\"], inplace=True)\n",
    "madd.to_parquet(sset.DIR_YPK_INT / \"maddison_project.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b195b",
   "metadata": {},
   "source": [
    "## UN WPP: overall populations data\n",
    "\n",
    "### Assign country (ISO) codes: initial try with obvious cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing data\n",
    "un_df = pd.read_csv(sset.DIR_YPK_RAW / \"UN_WPP2019_TotalPopulation.csv\")\n",
    "\n",
    "## let's check whether there are any with \"dependencies\" considered together with sov.s\n",
    "for i in set(un_df.Location):\n",
    "    if \"ependenc\" in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## very minor clean-up for iso country codes; initial\n",
    "countryname_to_iso = dict([])\n",
    "\n",
    "for i in list(set(un_df.Location)):\n",
    "    name = pyctry.countries.get(name=i)\n",
    "    oname = pyctry.countries.get(official_name=i)\n",
    "\n",
    "    if name is not None or oname is not None:\n",
    "        to_use = name\n",
    "        if name is None:\n",
    "            to_use = oname\n",
    "        countryname_to_iso[i] = to_use.alpha_3\n",
    "    else:\n",
    "        countryname_to_iso[i] = None\n",
    "\n",
    "## some mandotory clean-ups required\n",
    "## Will not print them as there are too many, but can be checked via print command\n",
    "## print(no_isos)\n",
    "no_isos = [k for k, v in countryname_to_iso.items() if v is None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## after examining the no_isos list, I conduct the following (manual) clean-up\n",
    "to_update = {\n",
    "    \"Micronesia (Fed. States of)\": \"FSM\",\n",
    "    \"State of Palestine\": \"PSE\",\n",
    "    \"China (and dependencies)\": \"CHN+D\",\n",
    "    \"China, Macao SAR\": \"MAC\",\n",
    "    \"China, Hong Kong SAR\": \"HKG\",\n",
    "    \"Bolivia (Plurinational State of)\": \"BOL\",\n",
    "    \"Saint Helena\": \"SHN\",\n",
    "    \"Holy See\": \"VAT\",\n",
    "    \"Venezuela (Bolivarian Republic of)\": \"VEN\",\n",
    "    \"Iran (Islamic Republic of)\": \"IRN\",\n",
    "    \"United Kingdom (and dependencies)\": \"GBR+D\",\n",
    "    \"New Zealand (and dependencies)\": \"NZL+D\",\n",
    "    \"Dem. People's Republic of Korea\": \"PRK\",\n",
    "    \"China, Taiwan Province of China\": \"TWN\",\n",
    "    \"Democratic Republic of the Congo\": \"COD\",\n",
    "    \"Republic of Korea\": \"KOR\",\n",
    "    \"United States Virgin Islands\": \"VIR\",\n",
    "    \"Denmark (and dependencies)\": \"DNK+D\",\n",
    "    \"France (and dependencies)\": \"FRA+D\",\n",
    "    \"United States of America (and dependencies)\": \"USA+D\",\n",
    "    \"Wallis and Futuna Islands\": \"WLF\",\n",
    "    \"Channel Islands\": \"GGY+JEY\",\n",
    "    \"Netherlands (and dependencies)\": \"NLD+D\",\n",
    "}\n",
    "\n",
    "## updating the ISO codes\n",
    "countryname_to_iso.update(to_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8189d5",
   "metadata": {},
   "source": [
    "### Detecting cases spanning multiple regions\n",
    "\n",
    "We do not want to account for cases like \"Europe\" where there are multiple countries / territories / sovereignties associated with it. Therefore, we will assign, to these multiple-region cases, the code `WIDE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2944f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## re-checking for clean-ups; again, this is too long a list to print\n",
    "no_isos_2 = [k for k, v in countryname_to_iso.items() if v is None]\n",
    "\n",
    "## the whole of no_isos_2 is \"WIDE\"\n",
    "for i, ctry in enumerate(no_isos_2):\n",
    "    countryname_to_iso[ctry] = \"WIDE\"\n",
    "\n",
    "## applying the dictionary to get country codes (ISO)\n",
    "un_df[\"ccode\"] = un_df.Location.map(countryname_to_iso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9644b04b",
   "metadata": {},
   "source": [
    "### Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(sset.DIR_YPK_INT, exist_ok=True)\n",
    "un_df.rename(columns={\"Time\": \"year\"}, inplace=True)\n",
    "un_df.set_index([\"ccode\", \"year\"], inplace=True)\n",
    "un_df.to_parquet(sset.DIR_YPK_INT / \"un_population.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a6c04",
   "metadata": {},
   "source": [
    "## UN WPP: population-by-age-group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "## attaching country codes; first import un_pop information\n",
    "by_age = pd.read_csv(sset.DIR_YPK_RAW / \"UN_WPP2019_Population_by_Age.csv\")\n",
    "\n",
    "## attaching the country codes\n",
    "un_df_dic = dict(zip(un_df.Location, un_df.index.get_level_values(\"ccode\")))\n",
    "by_age[\"ccode\"] = by_age.Location.map(un_df_dic)\n",
    "\n",
    "## double checking if any are missing country codes\n",
    "print(\"The missing-ccode rows are:\", by_age[pd.isnull(by_age.ccode)].shape[0])\n",
    "\n",
    "## saving the ccodes as indices\n",
    "by_age.set_index([\"ccode\"], inplace=True)\n",
    "\n",
    "## exporting\n",
    "by_age.to_parquet(sset.DIR_YPK_INT / \"un_population_by_age.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3eb43d",
   "metadata": {},
   "source": [
    "## GEG-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster setup\n",
    "N_CLUSTER = 20\n",
    "cluster = gateway.new_cluster(worker_image=image_name, profile=\"micro\")\n",
    "client = cluster.get_client()\n",
    "cluster.scale(N_CLUSTER)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b94412",
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def clean_chunk(start, num, shp_path):\n",
    "    with fiona.open(shp_path, \"r\") as shp:\n",
    "        chunk = shp[start : (start + num)]\n",
    "    properties = pd.DataFrame((map(itemgetter(\"properties\"), chunk)))\n",
    "    geometries = list(map(itemgetter(\"geometry\"), chunk))\n",
    "    coordinates = pd.DataFrame(\n",
    "        map(itemgetter(\"coordinates\"), geometries), columns=[\"lon\", \"lat\"]\n",
    "    )\n",
    "    df = coordinates.merge(properties, left_index=True, right_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ef7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_GAR = sset.DIR_GEG15_RAW / \"gar-exp\"\n",
    "with fiona.open(DIR_GAR / \"gar_exp.shp\") as shp:\n",
    "    num_geoms = len(shp)\n",
    "\n",
    "data_chunked = []\n",
    "for ii in range(0, num_geoms, 1000):\n",
    "    data_chunked.append(clean_chunk(ii, 1000, str(DIR_GAR / \"gar_exp.shp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c6a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddf.from_delayed(data_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe52819",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(npartitions=16).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(sset.DIR_GEG15_INT, exist_ok=True)\n",
    "df.to_parquet(sset.DIR_GEG15_INT / \"gar_exp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)\n",
    "client.close()\n",
    "cluster.close()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d10ce3-91a6-45d6-89b0-7b13985f200d",
   "metadata": {},
   "source": [
    "## Unzip and process Landscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a3d46-ca5a-4ddc-9545-66420177cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial.process_landscan(\n",
    "    landscan_zip=sset.DIR_LANDSCAN_RAW / f\"{sset.LANDSCAN_VERS}.zip\",\n",
    "    dir_landscan_raw=sset.DIR_LANDSCAN_RAW / sset.LANDSCAN_VERS,\n",
    "    dir_landscan_int=sset.DIR_LANDSCAN_INT,\n",
    "    landscan_year=sset.LANDSCAN_YEAR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2ac64",
   "metadata": {},
   "source": [
    "## CIA World Factbook: gathering GDP PPP terms\n",
    "\n",
    "The information gathered through sources such as PWT, World Bank WDI, and OECD Regional data often lack GDP information about many of the smaller or disputed countries and territories. In order to account for these countries, we incorporate data from CIA World Factbook dataset which has not much year-to-year information but has more countries covered.\n",
    "\n",
    "### Unzipping and organizing the files\n",
    "\n",
    "Note that the cell directly below needs to be run **only once** since it is basically unzipping the `.7z` zip file and may take a long time to repeat over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unzipping: this may take a long time\n",
    "CIA_DIR, zip_file_name = sset.DIR_YPK_RAW, \"weekly_json.7z\"\n",
    "shutil.register_unpack_format(\"7zip\", [\".7z\"], unpack_7zarchive)\n",
    "shutil.unpack_archive(CIA_DIR / zip_file_name, CIA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20797d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ordering them by time (Earlier entries first)\n",
    "CIA_DIR_week = sset.DIR_YPK_RAW / \"weekly_json\"\n",
    "file_lst = np.sort(os.listdir(CIA_DIR_week))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df988e2",
   "metadata": {},
   "source": [
    "### Fetch necessary information from the individual `.json` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4132539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_gdp_fetcher(filename, location=CIA_DIR_week):\n",
    "    \"\"\"From weekly-scraped CIA World Factbook data (in json format), gather relevant GDP\n",
    "    information and save as a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        individual weekly-scraped CIA World Factbook data file name\n",
    "    location : Path\n",
    "        where the CIA World Factbook data are stored at\n",
    "\n",
    "    overall_dict : dict\n",
    "        information (in dictionary format) containing the countries' GDP information\n",
    "        (in purchasing power parity) and for which year(s) those information is provided\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with open(location / filename) as fp:\n",
    "        data = json.load(fp)\n",
    "    ctries = list(data[\"countries\"].keys())\n",
    "    ctries.sort()\n",
    "\n",
    "    note_phrase_1 = \"data are in \"\n",
    "    note_phrase_2 = \" dollars\"\n",
    "    note_phrase_3 = \" us dollars\"\n",
    "\n",
    "    overall_dict = dict([])\n",
    "    for c in ctries:\n",
    "\n",
    "        try:\n",
    "            info = data[\"countries\"][c][\"data\"][\"economy\"][\"gdp\"]\n",
    "            info = info[\"purchasing_power_parity\"]\n",
    "            note = info.get(\"note\")\n",
    "\n",
    "            base_yr = None\n",
    "            if note is not None:\n",
    "                note = note.lower()\n",
    "                if (note_phrase_1 in note) and (note_phrase_3 in note):\n",
    "                    note_ = note.split(note_phrase_1)[1]\n",
    "                    note_ = note_.split(note_phrase_3)[0]\n",
    "                    base_yr = int(note_[0:4])\n",
    "                elif (note_phrase_1 in note) and (note_phrase_2 in note):\n",
    "                    note_ = note.split(note_phrase_1)[1]\n",
    "                    note_ = note_.split(note_phrase_2)[0]\n",
    "                    base_yr = int(note_[0:4])\n",
    "            info_values = info.get(\"annual_values\")\n",
    "            if (info_values is not None) and (type(info_values) in [tuple, list]):\n",
    "                keys = []\n",
    "                values = []\n",
    "                for i in info_values:\n",
    "                    keys.append(int(i[\"date\"]))\n",
    "                    values.append((i[\"value\"], int(i[\"date\"])))\n",
    "                if base_yr is not None:\n",
    "                    values = [(x[0], base_yr) for x in values]\n",
    "                yr_dict = dict(zip(keys, values))\n",
    "                overall_dict[c] = yr_dict\n",
    "\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return overall_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## individual results of the file_gdp_fetcher function stored in a list\n",
    "lst_results = []\n",
    "for f in tqdm(file_lst):\n",
    "    lst_results.append(file_gdp_fetcher(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2d1e9",
   "metadata": {},
   "source": [
    "### Updating the individual dictionaries with the most recent information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_one_with_two(dict1, dict2):\n",
    "    \"\"\"For simple updating of dictionaries, from `dict2` onto `dict1` in order to make\n",
    "    sure that all relevant CIA World Factbook data are gathered\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dict1 : dict\n",
    "        dictionary to implement the updates onto\n",
    "    dict2 : dict\n",
    "        dictionary to gather new information from\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict_ : dict\n",
    "        updated dictionary containing the information of both dictionaries\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dict_ = dict(dict1)\n",
    "    lst1 = list(dict1.keys())\n",
    "\n",
    "    for key in dict2.keys():\n",
    "        if key not in lst1:\n",
    "            dict_[key] = dict2[key]\n",
    "            continue\n",
    "\n",
    "        subdict = dict2[key]\n",
    "        subkeys = list(subdict.keys())\n",
    "        for subkey in subkeys:\n",
    "            dict_[key][subkey] = subdict[subkey]\n",
    "\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6aa475",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for res in tqdm(lst_results[1:]):\n",
    "    if i == 0:\n",
    "        midres = update_one_with_two(lst_results[0], res)\n",
    "    else:\n",
    "        midres = update_one_with_two(midres, res)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86819a",
   "metadata": {},
   "source": [
    "### Saving into a long-panel format dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ddf2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctry_dfs = []\n",
    "for i in midres.keys():\n",
    "    info = midres[i]\n",
    "    i_k = list(info.keys())\n",
    "    i_v = [info[i_k_] for i_k_ in i_k]\n",
    "    ctry_info = [[i, i_k[l]] + list(i_v[l]) for l in range(len(i_k))]\n",
    "    ctry_df = pd.DataFrame(ctry_info, columns=[\"country\", \"year\", \"gdp\", \"ppp_year\"])\n",
    "    ctry_dfs.append(ctry_df)\n",
    "ctry_agg_df = pd.concat(ctry_dfs, axis=0)\n",
    "ctry_agg_df[\"country\"] = [x.replace(\"_\", \" \") for x in ctry_agg_df[\"country\"]]\n",
    "ctry_agg_df.set_index([\"country\", \"year\"], inplace=True)\n",
    "ctry_agg_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabdcdca",
   "metadata": {},
   "source": [
    "### Assigning countrycodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b9a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's use the UN populations data, since it should have the most countries\n",
    "## to match names with values\n",
    "un_loc = sset.DIR_YPK_INT\n",
    "unpop = pd.read_parquet(un_loc / \"un_population.parquet\").reset_index()\n",
    "\n",
    "unpop[\"Location_lower\"] = [x.lower() for x in unpop.Location]\n",
    "initial_cleanup = dict(zip(unpop.Location_lower, unpop.ccode))\n",
    "\n",
    "## attaching the cleaned countrycodes\n",
    "initial_df = [list(initial_cleanup.keys()), list(initial_cleanup.values())]\n",
    "initial_df = pd.DataFrame(\n",
    "    np.array(initial_df).T, columns=[\"country\", \"ccode\"]\n",
    ").set_index([\"country\"])\n",
    "ctry_agg_df = ctry_agg_df.merge(\n",
    "    initial_df, left_index=True, right_index=True, how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2eeaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking which didn't get country codes\n",
    "cia_ccodes_only = ctry_agg_df.reset_index()[[\"country\", \"ccode\"]].drop_duplicates()\n",
    "unknown_case = []\n",
    "for i, case in enumerate(cia_ccodes_only[\"ccode\"]):\n",
    "    if pd.isnull(case):\n",
    "        unknown_case.append(cia_ccodes_only[\"country\"].values[i])\n",
    "unknown_case = np.sort(np.unique(unknown_case))\n",
    "print(unknown_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab6757",
   "metadata": {},
   "outputs": [],
   "source": [
    "## manual cleanup\n",
    "unknown_case_ccodes = [\"BHS\", \"BOL\", \"BRN\", \"MMR\", \"CPV\", \"COD\", \"COG\", \"CIV\", \"CUW\"]\n",
    "unknown_case_ccodes += [\"CZE\", \"TLS\", \"-\", \"FLK\", \"GMB\", \"-\", \"GGY\", \"GNB\", \"HKG\"]\n",
    "unknown_case_ccodes += [\"IRN\", \"JEY\", \"PRK\", \"KOR\", \"KO-\", \"LAO\", \"MAC\", \"MKD\", \"FSM\"]\n",
    "unknown_case_ccodes += [\"MDA\", \"-\", \"RUS\", \"SHN\", \"MAF\", \"SXM\", \"SWZ\", \"SYR\", \"TWN\"]\n",
    "unknown_case_ccodes += [\"TZA\", \"TLS\", \"USA\", \"VEN\", \"VNM\", \"VIR\", \"WLF\", \"-\"]\n",
    "\n",
    "## double-checking the names' lengths\n",
    "print(len(unknown_case) == len(unknown_case_ccodes))\n",
    "\n",
    "## getting a dataframe\n",
    "update_df = pd.DataFrame(data={\"country\": unknown_case, \"ccode2\": unknown_case_ccodes})\n",
    "update_df.set_index([\"country\"], inplace=True)\n",
    "ctry_agg_df = ctry_agg_df.merge(\n",
    "    update_df, left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "ctry_agg_df.loc[pd.isnull(ctry_agg_df.ccode), \"ccode\"] = ctry_agg_df.loc[\n",
    "    pd.isnull(ctry_agg_df.ccode), \"ccode2\"\n",
    "].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6631b61c",
   "metadata": {},
   "source": [
    "### Fetching the PPP conversion rates (to constant 2017 PPP USD), and applying the conversion rates\n",
    "\n",
    "Also, turn it into millions of USD (currently in ones of USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c1ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp_to_17 = ypk_fn.ppp_conversion_specific_year(2017, to=True, extrap_sim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2edb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## neutral assumption when conversion rates are missing\n",
    "ctry_agg_df = (\n",
    "    ctry_agg_df.reset_index()\n",
    "    .set_index([\"ccode\", \"year\"])\n",
    "    .drop([\"ccode2\"], axis=1)\n",
    "    .merge(ppp_to_17, left_index=True, right_index=True, how=\"left\")\n",
    ")\n",
    "ctry_agg_df.loc[pd.isnull(ctry_agg_df.conv), \"conv\"] = 1\n",
    "\n",
    "## first, divide by 1000000\n",
    "ctry_agg_df[\"gdp\"] = ctry_agg_df[\"gdp\"] / 1000000\n",
    "\n",
    "## applying the conversion by multiplying\n",
    "ctry_agg_df[\"gdp_ppp2017_currUSD\"] = ctry_agg_df[\"gdp\"] * ctry_agg_df[\"conv\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4de36",
   "metadata": {},
   "source": [
    "### Attaching the US deflators and generating constant 2017 PPP USD values\n",
    "\n",
    "Note that while they are now in PPP of 2017, they are yet to be turned into constant 2017 PPP (since they are in current USD, for many). Therefore, we will need to fetch the US deflators (using `pl_gdpo` from PWT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e9dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwt = pd.read_excel(sset.PATH_PWT_RAW).rename(columns={\"countrycode\": \"ccode\"})\n",
    "pwt.set_index([\"ccode\", \"year\"], inplace=True)\n",
    "\n",
    "us_defla = (\n",
    "    pwt.loc[\"USA\", [\"pl_gdpo\"]]\n",
    "    .reset_index()\n",
    "    .rename(columns={\"pl_gdpo\": \"pl_usa\", \"year\": \"ppp_year\"})\n",
    ")\n",
    "ctry_agg_df = (\n",
    "    ctry_agg_df.reset_index()\n",
    "    .merge(us_defla, on=[\"ppp_year\"], how=\"left\")\n",
    "    .set_index([\"ccode\", \"year\"])\n",
    ")\n",
    "\n",
    "## generating constant 2017 ppp\n",
    "ctry_agg_df[\"gdp_constant2017ppp\"] = (\n",
    "    ctry_agg_df[\"gdp_ppp2017_currUSD\"] / ctry_agg_df[\"pl_usa\"]\n",
    ")\n",
    "\n",
    "ctry_agg_df_reorg = ctry_agg_df[[\"gdp_constant2017ppp\", \"country\"]].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fed947",
   "metadata": {},
   "source": [
    "### Checking for redundancies in country (ISO) codes\n",
    "\n",
    "Except when there aren't any country-codes attached, these redundancies are occurring because there have been changes to the countries' names over the years or there are multiple names for one country. We will use the following rule to remove some of the overlaps:\n",
    "- SHN: Take only `saint helena ascension and tristan da cunha`\n",
    "- CZE: For 2006-2012, use `czech republic` information; for 2013 and onwards, use `czechia` information.\n",
    "- MKD: For 2006-2014, use `macedonia` information; for 2015 and onwards, use `north macedonia` information.\n",
    "- SWZ: For 2006-2014, use `swaziland` information; for 2015 and onwards, use `eswatini` information.\n",
    "- CPV: For 2006-2011, use `cape verde` information; for 2012 and onwards, use `cabo verde` information.\n",
    "- TLS: Take only `timor leste`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "reorg_ccodes = ctry_agg_df_reorg.reset_index()[[\"ccode\", \"country\"]].drop_duplicates()\n",
    "reorg_ccodes.set_index([\"ccode\"], inplace=True)\n",
    "for i, ccode in enumerate(np.unique(reorg_ccodes.index.values)):\n",
    "    countrycases = reorg_ccodes.loc[ccode, \"country\"]\n",
    "    if (ccode != \"-\") and (type(countrycases) != str):\n",
    "        print(ccode, countrycases.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_ones = [\"SHN\", \"CZE\", \"MKD\", \"SWZ\", \"CPV\", \"TLS\"]\n",
    "ctry_agg_df_redun = ctry_agg_df_reorg.reset_index()\n",
    "\n",
    "ctry_shn = ctry_agg_df_redun.loc[\n",
    "    ctry_agg_df_redun.country == \"saint helena ascension and tristan da cunha\"\n",
    "].set_index([\"ccode\", \"year\"])\n",
    "\n",
    "ctry_cze = ctry_agg_df_redun.loc[\n",
    "    ((ctry_agg_df_redun.country == \"czechia\") & (ctry_agg_df_redun.year >= 2013))\n",
    "    | (\n",
    "        (ctry_agg_df_redun.country == \"czech republic\")\n",
    "        & (ctry_agg_df_redun.year <= 2012)\n",
    "    )\n",
    "].set_index([\"ccode\", \"year\"])\n",
    "\n",
    "ctry_mkd = ctry_agg_df_redun[\n",
    "    ((ctry_agg_df_redun.country == \"macedonia\") & (ctry_agg_df_redun.year <= 2014))\n",
    "    | (\n",
    "        (ctry_agg_df_redun.country == \"north macedonia\")\n",
    "        & (ctry_agg_df_redun.year >= 2015)\n",
    "    )\n",
    "].set_index([\"ccode\", \"year\"])\n",
    "\n",
    "ctry_swz = ctry_agg_df_redun[\n",
    "    ((ctry_agg_df_redun.country == \"swaziland\") & (ctry_agg_df_redun.year <= 2014))\n",
    "    | ((ctry_agg_df_redun.country == \"eswatini\") & (ctry_agg_df_redun.year >= 2015))\n",
    "].set_index([\"ccode\", \"year\"])\n",
    "\n",
    "ctry_cpv = ctry_agg_df_redun[\n",
    "    ((ctry_agg_df_redun.country == \"cape verde\") & (ctry_agg_df_redun.year <= 2011))\n",
    "    | ((ctry_agg_df_redun.country == \"cabo verde\") & (ctry_agg_df_redun.year >= 2012))\n",
    "].set_index([\"ccode\", \"year\"])\n",
    "\n",
    "ctry_tls = ctry_agg_df_redun.loc[\n",
    "    ctry_agg_df_redun.country == \"timor leste\", :\n",
    "].set_index([\"ccode\", \"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctry_agg_df_final = ctry_agg_df_reorg[\n",
    "    ~ctry_agg_df_reorg.index.get_level_values(\"ccode\").isin(\n",
    "        [\"-\", \"WIDE\"] + redundant_ones\n",
    "    )\n",
    "].copy()\n",
    "\n",
    "ctry_agg_df_final = pd.concat(\n",
    "    [ctry_agg_df_final, ctry_shn, ctry_cze, ctry_mkd, ctry_swz, ctry_cpv, ctry_tls],\n",
    "    axis=0,\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633df76b",
   "metadata": {},
   "source": [
    "### Adding those that are not in the files\n",
    "\n",
    "**Tokelau `TKL`**\n",
    "\n",
    "According to Tokelau government (link [here](https://www.tokelau.org.nz/Bulletin/April+2017/GDP+first.html)), its PPP USD was 10 million (in 2017). So we will fill this in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c760d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkl = pd.DataFrame(\n",
    "    [\n",
    "        [\"TKL\", 2017, 10, \"tokelau\"],\n",
    "    ],\n",
    "    columns=[\"ccode\", \"year\", \"gdp_constant2017ppp\", \"country\"],\n",
    ").set_index([\"ccode\", \"year\"])\n",
    "ctry_agg_df_final = pd.concat([ctry_agg_df_final, tkl], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b2ec2f",
   "metadata": {},
   "source": [
    "**Saint Helena (`SHN`)**\n",
    "\n",
    "I update the latest values using the CIA World Factbook's January 7, 2021 vintage (link [here](https://www.cia.gov/the-world-factbook/)). For `SHN`, it is said that the 2009 value of GDP (in constant 2009 PPP USD) is 31.1 million, but we do not have the explicit PPP conversion for `SHN`. Since `SHN` is a British territory, `GBR` PPP rates are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shn_rate = ppp_to_17.loc[(\"SHN\", 2009), \"conv\"]\n",
    "us_def09 = pwt.loc[(\"USA\", 2009), \"pl_gdpo\"]\n",
    "shn = pd.DataFrame(\n",
    "    [\n",
    "        [\"SHN\", 2009, shn_rate / us_def09 * 31.1, \"saint helena\"],\n",
    "    ],\n",
    "    columns=[\"ccode\", \"year\", \"gdp_constant2017ppp\", \"country\"],\n",
    ").set_index([\"ccode\", \"year\"])\n",
    "\n",
    "ctry_agg_df_final = pd.concat([ctry_agg_df_final, shn], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207fa442",
   "metadata": {},
   "source": [
    "**Vatican (`VAT`)**\n",
    "\n",
    "While not in the latest CIA World Factbook, the 2000 version has some information about Vatican city (archived [here](https://www.encyclopedia.com/places/spain-portugal-italy-greece-and-balkans/italian-political-geography/vatican-city)) which we will be able to use. It says that the 1999 estimate of the Vatican GDP (assuming it's constant 1999 PPP) was 21 million USD. Let us use the PPP conversion rates of Italy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554deb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vat_rate = ppp_to_17.loc[(\"VAT\", 1999), \"conv\"]\n",
    "us_def99 = pwt.loc[(\"USA\", 1999), \"pl_gdpo\"]\n",
    "vat = pd.DataFrame(\n",
    "    [\n",
    "        [\"VAT\", 1999, vat_rate / us_def99 * 21, \"vatican\"],\n",
    "    ],\n",
    "    columns=[\"ccode\", \"year\", \"gdp_constant2017ppp\", \"country\"],\n",
    ").set_index([\"ccode\", \"year\"])\n",
    "\n",
    "ctry_agg_df_final = pd.concat([ctry_agg_df_final, vat], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b6c8c",
   "metadata": {},
   "source": [
    "### Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad9b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctry_agg_df_final.sort_index(inplace=True)\n",
    "ctry_agg_df_final.rename(columns={\"gdp_constant2017ppp\": \"cia_rgdpna\"}, inplace=True)\n",
    "ctry_agg_df_final.to_parquet(\n",
    "    sset.DIR_YPK_INT / \"cia_wf_gdp_constant_2017_ppp_usd_ver.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215170af-1b5f-448a-afbe-640f022474de",
   "metadata": {},
   "source": [
    "## Credit Suisse Global Wealth Databook 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4230e2e-01a5-4546-95fc-6604bd2539f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the file\n",
    "GWDB_NAME = \"global-wealth-databook-2021.pdf\"\n",
    "GWDB2021 = pypdf.PdfFileReader(open(str(sset.DIR_GLOBAL_WEALTH_RAW / GWDB_NAME), \"rb\"))\n",
    "\n",
    "# pages 25 to 108 hold relevant information for our workflow\n",
    "GWDB2021_org = []\n",
    "for l in tqdm(range(25, 109)):\n",
    "    page_text_info = GWDB2021.getPage(l - 1).extractText()\n",
    "\n",
    "    # relevant start country/region and end country/region\n",
    "    div = (l + 3) % 4\n",
    "    if div == 1:\n",
    "        start_c, end_c = \"Egypt\", \"Lithuania\"\n",
    "    elif div == 2:\n",
    "        start_c, end_c = \"Luxembourg\", \"Seychelles\"\n",
    "    elif div == 3:\n",
    "        start_c, end_c = \"Sierra Leone\", \"World\"\n",
    "    else:\n",
    "        start_c, end_c = \"Afghanistan\", \"Ecuador\"\n",
    "    GWDB2021_org.append(ypk_fn.organize_gwdb_2021_page(page_text_info, start_c, end_c))\n",
    "GWDB2021_org = pd.concat(GWDB2021_org, axis=0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8af62d-0488-4cce-93b2-47e3bef6bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "GWDB2021_org = pd.read_parquet(sset.DIR_GLOBAL_WEALTH_INT / \"gwdb_2021.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a9c663-b8f3-4bd3-b5ae-e918b4432341",
   "metadata": {},
   "source": [
    "## CIA World Factbook (WFB)\n",
    "\n",
    "Note that all WFB versions have variations in how files are organized and each version only contains at most 3 years' data; therefore, we clean (yearly) version by version.\n",
    "\n",
    "### WFB 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb321173-b70c-4c33-a389-cfc66af040da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_wfb2000, gdp_wfb2000 = ypk_fn.organize_cia_wfb_2000()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c21a2-0059-47f0-9820-6159296b594f",
   "metadata": {},
   "source": [
    "### WFB 2019 and 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3e59e-c6e9-41bf-91bf-2c219e31e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 and 2020 versions share similar format\n",
    "pop_wfb2019, gdp_wfb2019, gdppc_wfb2019 = ypk_fn.organize_cia_wfb_2019_2020(\n",
    "    directory=(sset.DIR_CIA_RAW / \"factbook-2019\"), wfb_year=2019\n",
    ")\n",
    "\n",
    "pop_wfb2000, gdp_wfb2000, gdppc_wfb2000 = ypk_fn.organize_cia_wfb_2019_2020()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
